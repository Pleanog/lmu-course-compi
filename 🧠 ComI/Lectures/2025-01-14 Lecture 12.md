### **Policy-Based vs. Value-Based**

#### **Policy-Based Methods**

- Directly learn the optimal policy π(a∣s)\pi(a|s)π(a∣s), which maps states (sss) to actions (aaa).
- Do not use a value function explicitly.
- Suitable for problems with continuous action spaces or where stochastic policies are needed.
- Example: REINFORCE, PPO.

##### Value-Based Methods

- Learn a value function Q(s,a)Q(s, a)Q(s,a) that estimates the expected reward of taking action aaa in state sss.
- The policy is indirectly derived by selecting actions that maximize the value function (e.g., π(s)=arg⁡max⁡Q(s,a)\pi(s) = \arg\max Q(s, a)π(s)=argmaxQ(s,a)).
- Typically used for discrete action spaces.
- Example: Q-Learning, DQN.

##### Key Differences

|Aspect|Policy-Based|Value-Based|
|---|---|---|
|Learning Target|Directly learns policy (\pi(a|s))|
|Action Space|Handles continuous/stochastic actions|Best suited for discrete actions|
|Convergence Stability|May converge to a local optimum|Can struggle with convergence stability (e.g., divergence)|
|Examples|PPO, REINFORCE|Q-Learning, DQN|

---

### **On-Policy vs. Off-Policy**

#### **On-Policy Methods**

- Learn the policy that is being followed during the data collection (training phase).
- Policies improve based on the experiences generated by the current policy.
- Example: SARSA, PPO.

##### Off-Policy Methods

- Learn the optimal policy while using data collected by a different policy.
- Can reuse past experiences (from a replay buffer), allowing for better sample efficiency.
- Example: Q-Learning, DDPG.

##### Key Differences

|Aspect|On-Policy|Off-Policy|
|---|---|---|
|Data Collection|Uses experiences from the current policy|Can use experiences from any policy|
|Sample Efficiency|Lower (no reusing of experiences)|Higher (reuses past data effectively)|
|Stability|Easier to stabilize during training|Can be harder to stabilize due to policy mismatch|
|Examples|SARSA, PPO|Q-Learning, DDPG|

---

### **Online vs Offline**

#### **Online RL**

- The agent updates its policy (or value function) while interacting with the environment during the episode.
- Feedback is used immediately to refine the policy.
- Example: A robot learning how to navigate a labyrinth and adapting its behavior as it moves.
- Often paired with **on-policy** methods but can also work with off-policy methods.

#### **Offline RL**

- The agent learns from a fixed dataset of pre-collected experiences without interacting with the environment during training.
- No real-time policy updates are made while the episode is running.
- Example: Training a policy for a robot using previously recorded labyrinth navigation data.
- Often used with **off-policy** methods to leverage past experiences effectively.

---

#### **Key Differences Between Online and Offline RL**

| Aspect         | Online RL                                           | Offline RL                                                 |
| -------------- | --------------------------------------------------- | ---------------------------------------------------------- |
| Policy Updates | During the episode                                  | After the episode (or entirely separate from it)           |
| Interaction    | Requires real-time interaction with the environment | No direct interaction with the environment during training |
| Data Usage     | Uses live data (collected on-the-fly)               | Uses pre-collected, fixed data                             |
| Efficiency     | Can adapt dynamically to the environment            | Highly sample-efficient but requires good-quality data     |
| Examples       | On-policy SARSA, online Q-learning                  | Batch Q-learning, offline DDPG                             |

##### **Relation to On-Policy vs Off-Policy**

- **Online RL** is naturally suited to **on-policy** methods since the policy improves based on real-time experiences. However, **off-policy** methods can also be used online with techniques like replay buffers.
- **Offline RL** works best with **off-policy** methods since it relies on data collected from different policies. On-policy methods struggle in offline RL as they require experiences directly tied to the current policy.

---
---

# Multi-Agent Applications

![[Pasted image 20250114123839.png]]

### Single Agents vs Multi-Agents:

- **Single Agent:** An autonomous system or entity with individual goals, designed to act and make decisions independently.
- **Multi-Agent:** A system consisting of multiple interacting agents, where the goals are often defined on a system-wide level, requiring coordination or emergent behavior.

|                          | **Single Agent Programming**     | **Multi-Agent Programming**\|         |
| ------------------------ | -------------------------------- | ------------------------------------- |
| **Single Agent Goals**\| | Single-agent system              | Coordination (e.g., game theory)      |
| **Single Agent Goals**   | Emergent behavior (e.g., swarms) | Agent groups, societies, institutions |

---

### Explanation of Overlaps:

- **Emergent Behavior:**  
    In multi-agent systems, each agent may have its own individual goal, but their combined interactions can lead to achieving a larger, system-wide goal without explicitly defining it. For example, in a flock of birds, each bird follows simple local rules, but together they form a cohesive swarm.
    
- **Example – Power Grid Operators:**  
    Power grid operators are a real-world example of combining single-agent and multi-agent goals. Each operator (agent) seeks to maximize profit (an individual goal) while minimizing system failures (a system-wide goal). This creates a blend of individual incentives and emergent cooperation, resulting in a stable power grid that satisfies both local and global objectives.


### Why?

1. **Resilience and Efficiency**: Swarms can be more effective in certain scenarios due to their redundancy and lack of single points of failure. If one agent fails, others can continue to operate, ensuring robustness. Additionally, swarms can often complete tasks more efficiently through distributed work and parallel processing, especially in dynamic or large-scale environments.
    
2. **When Multi-Agent Systems Are Inevitable**: Multi-agent systems are essential in scenarios where agents must interact with humans or other independent entities. For example, when robots interact with humans, a multi-agent system is inherently formed. While the human and robot may share the same overall goal (e.g., completing a task together), their individual goals or approaches might differ. This requires coordination, negotiation, and adaptability to achieve a harmonious outcome.
    

**Additional Notes**:  
Multi-agent systems often offer advantages in complex environments where adaptability and collaboration are key. For instance:

- In disaster response, swarms of drones can search large areas more quickly than a single drone.
- In social settings, robots working alongside humans must accommodate human behaviors, requiring a shared understanding of goals and priorities while respecting individual autonomy.


![[Pasted image 20250114124910.png]]

This graph is explained more or less in [[Definition 12 (multi-agent system)]]

---
# Multi-Agent Systems as a Paradigm for Distributed Programming

$$
\begin{align*}
\text{GreetingRobot}(\_) &= \overline{\text{good\_morning}}.\text{GreetingRobot}(\text{"happy"}) + \text{morning.Greeting.Robot}(\text{"happy"}) \\
\text{HappyMultiAgentSystem} &= \text{GreetingRobot}(\text{"sad"}) \mid \overline{\text{good\_morning}}.\text{GreetingRobot}(\text{"angry"})
\end{align*}

$$

### ELI5

Robots working together to say "good morning" or "greeting messages" in a team.

- The **`GreetingRobot`** function decides how a robot greets. It can take different "moods" like happy or sad.
- The **`HappyMultiAgentSystem`** combines greetings, mixing different moods from the GreetingRobot and adding modifiers (like ignoring "good morning" or emphasizing it with an overline).

We are defining how robots decide to greet and how they work as a system to greet in various ways!

---

# Rock-Paper-Scissors

A **multi-agent system** can be exemplified by a two-player **Rock-Paper-Scissors** game. Here, each player has their own **action space**, denoted as $A^{[1]}$ for Player 1 and $A^{[2]}$ for Player 2, with possible actions: Rock (R), Paper (P), and Scissors (S).

The game's outcome depends on the combination of actions chosen by the players, represented in a payoff table. For instance:

- If Player 1 chooses Scissors ($S$) and Player 2 chooses Rock ($R$), the payoff vector could be $\chi(S, R) = (-1, +1)$, meaning Player 1 loses and Player 2 wins.

The **payoff function** $\chi$ maps every action pair $(a^{[1]}, a^{[2]}) \in A^{[1]} \times A^{[2]}$ to a vector, such as $(+1, -1)$, indicating gains or losses for each player. This simple game structure highlights the interaction and interdependence of agents in a competitive environment.

![[Pasted image 20250114134621.png]]
